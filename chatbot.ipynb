{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import jieba\n",
    "import nltk\n",
    "import itertools\n",
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 30000\n",
    "vocabulary_path = 'vocab/vocab-list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27241\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for i in range(1, 8):\n",
    "    for j in range(1, 10):\n",
    "        try:\n",
    "            with codecs.open(\"subtitle/%d-%d.srt\" % (i, j), \"r\", \"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "                startLine = False\n",
    "                tmp = []\n",
    "                for line in lines:\n",
    "                    if re.search(r\"^[\\n|\\t|\\r]+$\", line):\n",
    "                        startLine = False\n",
    "                        if tmp:\n",
    "                            sentences.append(u','.join(tmp))\n",
    "                        tmp = []\n",
    "                    if startLine:\n",
    "                        #print line\n",
    "                        if not re.search(u\"[<|>|（|）|(|)|：|:|《|》|·|*|■|^\\d+$|www\\.|季第|微信掃描|^權力的游戲$|論壇|字幕組|榮譽出品|^謝謝觀賞$]\", line):\n",
    "                            line = re.sub(u\"[-|…|「|」|♪|~|！|\\\"|\\'|?|？|]+\", '', line)\n",
    "                            line = re.sub(u\"\\s+\", ',', line)\n",
    "                            if line and not (re.search(r\"^[\\n|\\t|\\r]+$\", line)):\n",
    "                                tmp.append(line)\n",
    "                    if re.search(\"-->\", line):\n",
    "                        startLine = True\n",
    "        except IOError:\n",
    "            break\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.208 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "sentences_tokenize = [jieba.lcut(sentence) for sentence in sentences]\n",
    "word_freq = nltk.FreqDist(itertools.chain(*sentences_tokenize))\n",
    "vocab = word_freq.most_common(vocabulary_size-4)\n",
    "vocab_list = _START_VOCAB + [word[0] for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not gfile.Exists(vocabulary_path):\n",
    "    with gfile.GFile(vocabulary_path, mode=\"wb\") as gf:\n",
    "        for w in vocab_list:\n",
    "            gf.write(w + b\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index = dict([(w, i) for i, w in enumerate(vocab_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence=別怕|，|馬|兒|,, ids=[2638, 8, 138, 676, 4]\n"
     ]
    }
   ],
   "source": [
    "sentences_ids = []\n",
    "for sentence in sentences_tokenize:\n",
    "    sentence_ids = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_index:\n",
    "            sentence_ids.append(word_to_index[word])\n",
    "        else:\n",
    "            sentence_ids.append(UNK_ID)\n",
    "    sentences_ids.append(sentence_ids)\n",
    "print \"sentence=%s, ids=%s\" % (\"|\".join(sentences_tokenize[0]), sentences_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1_length: 13620, p2_length: 13620\n",
      "p1_sample: [587, 386, 9903, 11, 11762, 329, 4], p2_sample: [5521, 4]\n"
     ]
    }
   ],
   "source": [
    "p1 = [sentences_ids[i] for i in xrange(0, len(sentences_ids), 2)]\n",
    "p2 = [sentences_ids[i] for i in xrange(1, len(sentences_ids), 2)]\n",
    "minLength = min(len(p1), len(p2))\n",
    "p1 = p1[:minLength]\n",
    "p2 = p2[:minLength]\n",
    "print \"p1_length: %d, p2_length: %d\" % (len(p1), len(p2))\n",
    "print \"p1_sample: %s, p2_sample: %s\" % (p1[5], p2[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_buckets = [(5, 10), (10, 15), (20, 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data_set=[[6571, 840, 20, 49, 4], [956, 11, 35, 840, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "data_set = [[] for _ in _buckets]\n",
    "for idx in range(len(p1)):\n",
    "    for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "        if len(p1[idx]) < source_size and len(p2[idx])+1 < target_size:\n",
    "            data_set[bucket_id].append([p1[idx], p2[idx]+[EOS_ID]])\n",
    "            break\n",
    "print \"Sample data_set=%s\" % data_set[1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, \n",
    "                 usize, \n",
    "                 vocab_size, \n",
    "                 num_layers, \n",
    "                 buckets, \n",
    "                 max_gradient_norm, \n",
    "                 batch_size, \n",
    "                 learning_rate, \n",
    "                 learning_rate_decay_factor, \n",
    "                 use_lstm=False, \n",
    "                 forward_only=False, \n",
    "                 num_samples=512, \n",
    "                 dtype=tf.float32):\n",
    "        self.buckets = buckets\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=dtype)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        # 使用sampled softmax時，需要 output projection (不清楚)\n",
    "        output_projection = None\n",
    "        softmax_loss_function = None\n",
    "        \n",
    "        if num_samples > 0 and num_samples < self.vocab_size:\n",
    "            w_t = tf.get_variable(\"proj_w\", [self.vocab_size, usize], dtype=dtype)\n",
    "            w = tf.transpose(w_t)\n",
    "            b = tf.get_variable(\"proj_b\", [self.vocab_size], dtype=dtype)\n",
    "            output_projection = (w, b)\n",
    "            \n",
    "            def sampled_loss(labels, logits):\n",
    "                labels = tf.reshape(labels, [-1, 1])\n",
    "                local_w_t = tf.cast(w_t, tf.float32)\n",
    "                local_b = tf.cast(b, tf.float32)\n",
    "                local_inputs = tf.cast(logits, tf.float32)\n",
    "                return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(\n",
    "                        weights=local_w_t,\n",
    "                        biases=local_b,\n",
    "                        labels=labels,\n",
    "                        inputs=local_inputs,\n",
    "                        num_sampled=num_samples,\n",
    "                        num_classes=self.vocab_size), dtype)\n",
    "            \n",
    "            softmax_loss_function = sampled_loss\n",
    "        \n",
    "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            # 建立 multi-layer cell 給 RNN 用\n",
    "            def single_cell():\n",
    "                return tf.contrib.rnn.GRUCell(usize)\n",
    "            if use_lstm:\n",
    "                def single_cell():\n",
    "                    return tf.contrib.rnn.BasicLSTMCell(usize)\n",
    "            cell = single_cell()\n",
    "\n",
    "            if num_layers > 1:\n",
    "                cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n",
    "            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                encoder_inputs,\n",
    "                decoder_inputs,\n",
    "                cell,\n",
    "                num_encoder_symbols=vocab_size,\n",
    "                num_decoder_symbols=vocab_size,\n",
    "                embedding_size=usize,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=do_decode,\n",
    "                dtype=dtype)\n",
    "        \n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.target_weights = []\n",
    "        # 最後一個bucket的 size 最大，取 source\n",
    "        for i in xrange(buckets[-1][0]):\n",
    "            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"encoder{0}\".format(i)))\n",
    "        \n",
    "        for i in xrange(buckets[-1][1]+1):\n",
    "            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"decoder{0}\".format(i)))\n",
    "            self.target_weights.append(tf.placeholder(dtype, shape=[None], name=\"weight{0}\".format(i)))\n",
    "        \n",
    "        # 往右移一位，估計是要保留給 \"GO\" 符號\n",
    "        targets = [self.decoder_inputs[i+1] for i in xrange(len(self.decoder_inputs)-1)]\n",
    "        \n",
    "        if forward_only:\n",
    "            self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
    "                softmax_loss_function=softmax_loss_function)\n",
    "            if output_projection is not None:\n",
    "                for b in xrange(len(buckets)):\n",
    "                    self.outputs[b] = [\n",
    "                        tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "                        for output in self.outputs[b]\n",
    "                    ]\n",
    "        else:\n",
    "            self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, False),\n",
    "                softmax_loss_function=softmax_loss_function)\n",
    "        \n",
    "        # Gradients and SGD update\n",
    "        params = tf.trainable_variables()\n",
    "        if not forward_only:\n",
    "            self.gradient_norms = []\n",
    "            self.updates = []\n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            for b in xrange(len(buckets)):\n",
    "                gradients = tf.gradients(self.losses[b], params)\n",
    "                clipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "                self.gradient_norms.append(norm)\n",
    "                self.updates.append(opt.apply_gradients(\n",
    "                    zip(clipped_gradients, params), global_step=self.global_step))\n",
    "                \n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only):\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        # Input feed\n",
    "        input_feed = {}\n",
    "        for l in xrange(encoder_size):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "        for l in xrange(decoder_size):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "            input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "        \n",
    "        # 因為已經將 target weight 作 shift 1, 所以需要補 1 個\n",
    "        last_target = self.decoder_inputs[decoder_size].name\n",
    "        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "        \n",
    "        # Output feed\n",
    "        if not forward_only:\n",
    "            output_feed = [self.updates[bucket_id], self.gradient_norms[bucket_id], self.losses[bucket_id]]\n",
    "        else:\n",
    "            output_feed = [self.losses[bucket_id]]\n",
    "            for l in xrange(decoder_size):\n",
    "                output_feed.append(self.outputs[bucket_id][l])\n",
    "        \n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if not forward_only:\n",
    "            return outputs[1], outputs[2], None # Gradient norm, loss, no outputs.\n",
    "        else:\n",
    "            return None, outputs[0], outputs[1:] # No gradient norm, loss, outputs.\n",
    "    \n",
    "    def get_batch(self, data, bucket_id):\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        encoder_inputs, decoder_inputs = [], []\n",
    "        \n",
    "        # 根據 batch_size 大小 隨機從 data bucket中挑選 encoder 及 decoder 的 inputs\n",
    "        for _ in xrange(self.batch_size):\n",
    "            encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "            \n",
    "            # Encoder inputs 放上 \"PAD\" 符號 以及 reverse\n",
    "            encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n",
    "            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "            \n",
    "            # Decoder inputs 前後插上 \"GO\" 以及 \"PAD\"\n",
    "            decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "            decoder_inputs.append([GO_ID] + decoder_input + [PAD_ID] * decoder_pad_size)\n",
    "        \n",
    "        # 建立 batch vector from above data\n",
    "        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "        \n",
    "        # Batch encoder inputs re-indexed encoder_inputs\n",
    "        for length_idx in xrange(encoder_size):\n",
    "            batch_encoder_inputs.append(\n",
    "                np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                         for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "        \n",
    "        # Batch decoder input re-indexed decoder_inputs, 並建立 weights\n",
    "        for length_idx in xrange(decoder_size):\n",
    "            batch_decoder_inputs.append(\n",
    "                np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                         for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "            \n",
    "            # 當遇到 \"PAD\" 符號時，target weight 歸 0\n",
    "            batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "            for batch_idx in xrange(self.batch_size):\n",
    "                # 記住，將 decoder inputs 作 shift 1\n",
    "                if length_idx < decoder_size-1:\n",
    "                    target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "                if length_idx == decoder_size-1 or target == PAD_ID:\n",
    "                    batch_weight[batch_idx] = 0.0\n",
    "            batch_weights.append(batch_weight)\n",
    "        return batch_encoder_inputs, batch_decoder_inputs, batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(session, train_dir, usize, vocab_size, num_layers, buckets, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm, forward_only):\n",
    "    dtype = tf.float32\n",
    "    model = Seq2SeqModel(usize, vocab_size, num_layers, buckets, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm, forward_only)\n",
    "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        print \"Reading model parameters from %s\" % ckpt.model_checkpoint_path\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print \"Create model with fresh parameters.\"\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data_set, steps_per_checkpoint, total_train_steps, train_dir, usize, vocab_size, num_layers, buckets, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm=False, forward_only=False):\n",
    "    with tf.Session() as session:\n",
    "        print \"Creating %d layers of %d units.\" % (num_layers, usize)\n",
    "        # 建立 model\n",
    "        model = create_model(session, \n",
    "                             train_dir, \n",
    "                             usize,\n",
    "                             vocab_size,\n",
    "                             num_layers, \n",
    "                             buckets, \n",
    "                             max_gradient_norm, \n",
    "                             batch_size, \n",
    "                             learning_rate, \n",
    "                             learning_rate_decay_factor, \n",
    "                             use_lstm, \n",
    "                             forward_only)\n",
    "        \n",
    "        train_bucket_sizes = [len(data_set[b]) for b in xrange(len(buckets))]\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "        \n",
    "        # 算出數值區間落在 [0~1] 的 bucket sacle，用來挑選訓練 bucket\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                             for i in xrange(len(train_bucket_sizes))]\n",
    "        \n",
    "        # Training loop !!!\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        while True:\n",
    "            if current_step == total_train_steps:\n",
    "                print \"Training finish at step: %d\" % current_step\n",
    "                break\n",
    "            # 隨機挑選 bucket 來訓練\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                            if train_buckets_scale[i] > random_number_01])\n",
    "            \n",
    "            # Get a batch and make a step\n",
    "            start_time = time.time()\n",
    "            \n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "            _, step_loss, _ = model.step(session, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "            \n",
    "            step_time += (time.time() - start_time) / steps_per_checkpoint\n",
    "            loss += step_loss / steps_per_checkpoint\n",
    "            current_step += 1\n",
    "            \n",
    "            if current_step % steps_per_checkpoint == 0:\n",
    "                perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "                print \"global step %d learning rate %.4f step-time %.2f perplexity %.2f\" \\\n",
    "                    % (model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity)\n",
    "                \n",
    "                # 如果超過 3 次 loss 沒改善的話，降低 learning rate\n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    session.run(model.learning_rate_decay_op)\n",
    "                \n",
    "                previous_losses.append(loss)\n",
    "                \n",
    "                # Save checkpoint and zero timer and loss.\n",
    "                checkpoint_path = os.path.join(train_dir, \"chatbot.ckpt\")\n",
    "                model.saver.save(session, checkpoint_path, global_step=model.global_step)\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                \n",
    "                # Run evals on development set and print their perplexity\n",
    "                for bucket_id in xrange(len(buckets)):\n",
    "                    if len(data_set[bucket_id]) == 0:\n",
    "                        print \" eval: empty bucket %d\" % bucket_id\n",
    "                        continue\n",
    "                    encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "                    _, eval_loss, _ = model.step(session, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "                    eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
    "                    print \" eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx)\n",
    "                sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_vocabulary(vocabulary_path):\n",
    "    if gfile.Exists(vocabulary_path):\n",
    "        rev_vocab = []\n",
    "        with gfile.GFile(vocabulary_path, mode=\"rb\") as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_token_ids(sentence, vocabulary):\n",
    "    words = jieba.lcut(sentence)\n",
    "    return [vocabulary.get(w.encode('utf-8'), UNK_ID) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(train_dir, usize, vocab_size, num_layers, buckets, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm=False, forward_only=True):\n",
    "    with tf.Session() as session:\n",
    "        model = create_model(session, \n",
    "                             train_dir, \n",
    "                             usize,\n",
    "                             vocab_size,\n",
    "                             num_layers, \n",
    "                             buckets, \n",
    "                             max_gradient_norm, \n",
    "                             batch_size, \n",
    "                             learning_rate, \n",
    "                             learning_rate_decay_factor, \n",
    "                             use_lstm, \n",
    "                             forward_only)\n",
    "        \n",
    "        model.batch_size = 1 # decode one sentence at a time\n",
    "        \n",
    "        # 載入 vocabulary\n",
    "        encoder_vocab, decoder_vocab = initialize_vocabulary(vocabulary_path)\n",
    "        \n",
    "        # Decode 使用者輸入的字串\n",
    "        sys.stdout.write(\"> \")\n",
    "        sys.stdout.flush()\n",
    "        sentence = sys.stdin.readline()\n",
    "        # TODO: Tokenize 和 mapping to ids\n",
    "        while sentence:\n",
    "            # 將 sentence 轉換成 ids 陣列\n",
    "            token_ids = sentence_to_token_ids(tf.compat.as_bytes(sentence), encoder_vocab)\n",
    "            bucket_id = len(buckets)-1\n",
    "            for i, bucket in enumerate(buckets):\n",
    "                if bucket[0] >= len(token_ids):\n",
    "                    bucket_id = i\n",
    "                    break\n",
    "                else:\n",
    "                    logging.warning(\"Sentence truncated: %s\" % sentence)\n",
    "            \n",
    "            # 取 1-element batch to feed sentence to model.\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "                {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "            \n",
    "            # 取 output logits for the sentence\n",
    "            _, _, output_logits = model.step(session, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "            \n",
    "            # Greedy decoder - outputs 只是 argmaxes of output_logits\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            \n",
    "            # output 以 \"EOS\" 符號為斷點\n",
    "            if EOS_ID in outputs:\n",
    "                outputs = outputs[:outputs.index(EOS_ID)]\n",
    "            # 印出 bot 回覆訊息\n",
    "            print \" \".join([tf.compat.as_str(decoder_vocab[output]) for output in outputs])\n",
    "            print \"> \"\n",
    "            sys.stdout.flush()\n",
    "            sentence = sys.stdin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 3 layers of 1024 units.\n",
      "Reading model parameters from train/chatbot.ckpt-6000\n",
      "INFO:tensorflow:Restoring parameters from train/chatbot.ckpt-6000\n"
     ]
    }
   ],
   "source": [
    "steps_per_checkpoint = 200\n",
    "train_dir = 'train/'\n",
    "total_train_steps = 6000\n",
    "usize=1024\n",
    "vocab_size=vocabulary_size\n",
    "num_layers=3\n",
    "max_gradient_norm=5.0\n",
    "batch_size=64\n",
    "learning_rate=0.5\n",
    "learning_rate_decay_factor=0.99\n",
    "use_lstm=True\n",
    "forward_only=False\n",
    "\n",
    "train(data_set, \n",
    "      steps_per_checkpoint,\n",
    "      total_train_steps,\n",
    "      train_dir, \n",
    "      usize,\n",
    "      vocab_size,\n",
    "      num_layers, \n",
    "      _buckets,\n",
    "      max_gradient_norm, \n",
    "      batch_size, \n",
    "      learning_rate, \n",
    "      learning_rate_decay_factor,\n",
    "      use_lstm, \n",
    "      forward_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_per_checkpoint = 200\n",
    "train_dir = 'train/'\n",
    "total_train_steps = 6000\n",
    "usize=1024\n",
    "vocab_size=vocabulary_size\n",
    "num_layers=3\n",
    "max_gradient_norm=5.0\n",
    "batch_size=1\n",
    "learning_rate=0.5\n",
    "learning_rate_decay_factor=0.99\n",
    "use_lstm=False\n",
    "forward_only=True\n",
    "\n",
    "decode(train_dir, \n",
    "       usize,\n",
    "       vocab_size, \n",
    "       num_layers, \n",
    "       _buckets, \n",
    "       max_gradient_norm, \n",
    "       batch_size, \n",
    "       learning_rate, \n",
    "       learning_rate_decay_factor, \n",
    "       use_lstm,\n",
    "       forward_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
